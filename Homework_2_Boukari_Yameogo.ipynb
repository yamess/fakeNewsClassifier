{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, copy, spacy, numpy as np, pandas as pd, os, pickle, time, datetime\n",
    "import torchtext, torch, torch.nn as nn, torch.optim, torch.nn.functional as F, torch.autograd as autograd, transformers\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import FastText\n",
    "from torch.optim import Adam, Adamax, AdamW, Adadelta, Adagrad\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, RandomSampler,SequentialSampler\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    BertModel, \n",
    "    BertForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities Functions and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing function\n",
    "def data_preprocess(x, y, ref):\n",
    "    \"\"\"\n",
    "    Preprocessing function That will, for each title, look for its references if it exists and concatenate them into\n",
    "    a single text\n",
    "    :param x: Text data which represent the titles\n",
    "    :param y: represente the either the train data or the test data ids\n",
    "    :param ref: Represente the reference for a given title ids\n",
    "    :return: A new dataframe with with a new column called concat_data which will used for the classification\n",
    "    \"\"\"\n",
    "    def ref_func(i, ref_df):\n",
    "        \"\"\"\n",
    "        Thin encapsulated function task is to look for all the references for a given title id and concat them into a\n",
    "        single text\n",
    "        :param i: Text id or title id\n",
    "        :param ref_df: The refertences dataframe\n",
    "        :return: Pandas dataframe with a new colum called new_var which the references concatenated into a single text\n",
    "        for a given title\n",
    "        \"\"\"\n",
    "        ref_list = list(ref_df[ref_df.id_x == i][\"title\"])\n",
    "        return \" \".join(ref_list)\n",
    "    df = y.merge(x, left_on=\"id\", right_on=\"id\", how=\"inner\")   # Merge the label and text into on df\n",
    "    ref_merge = ref.merge(x, left_on=\"id.1\", right_on=\"id\", how=\"left\")\n",
    "    df[\"new_var\"] = df.id.apply(lambda i: ref_func(i, ref_merge))\n",
    "    df[\"AllCombined\"] = df[\"title\"] + \" \" + df[\"new_var\"]\n",
    "    df.drop(\"new_var\", axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_grams(text):\n",
    "    \"\"\"\n",
    "    Function to create n_grams for a given text\n",
    "    :param text: Text data (tokens)\n",
    "    :param n: the n_grams parameters\n",
    "    :return: A liste of n_grams tokens\n",
    "    \"\"\"\n",
    "    n_grams = set(zip(*[text[i:] for i in range(2)]))\n",
    "    for gram in n_grams:\n",
    "        text.append(' '.join(gram))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "def spacy_tokenize(x):\n",
    "    return [tok.text for tok in tokenizer(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    \"\"\"\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    :param elapsed:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim,\n",
    "                 dropout):\n",
    "        \"\"\"\n",
    "        Convolution Neural Network Class for Text Classification\n",
    "        :param vocab_size: Vocabulary size\n",
    "        :param embedding_dim: The embedding dimension\n",
    "        :param n_filters: The number of filters to use\n",
    "        :param filter_sizes: The filters sizes to use. This define the n-gram to use\n",
    "        :param output_dim: The number of class to clssify\n",
    "        :param dropout: The dropout to use for regularisation\n",
    "        \"\"\"\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.n_filters = n_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                      out_channels=self.n_filters,\n",
    "                      kernel_size=[fs, embedding_dim], padding=(fs-1, 0))\n",
    "            for fs in self.filter_sizes\n",
    "        ])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(len(self.filter_sizes) * self.n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "        The forward pass function\n",
    "        :param text: The text data numericalised. The text Data is coming like [sent_len, batch_size]\n",
    "        :return: The logits corresponding to all the classes\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(text)  # --> [batch_size, sent_len, emb_dim]\n",
    "        embedded = embedded.unsqueeze(1)  # --> [batch size, 1, sent len, emb dim]\n",
    "        # embedded = self.dropout(embedded)\n",
    "        conved = [\n",
    "            self.relu(conv(embedded)).squeeze(3) for conv in self.convs\n",
    "        ]  # --> [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        pooled = [\n",
    "            F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved\n",
    "        ]  # --> [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))  # --> [batch size, n_filters * len(filter_sizes)]\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_layers, embedding_dim, hidden_dim, num_label, padding_idx, device,\n",
    "                 dropout=0.1, bidirectional=True):\n",
    "        \"\"\"\n",
    "        LSTM Model class for text classification with multilayer and bidirectionnal implementation\n",
    "        :param vocab_size: Vocabulary size (int)\n",
    "        :param num_layers: Number of layers (int)\n",
    "        :param embedding_dim: Embedding dimension (int)\n",
    "        :param hidden_dim: Hidden dimension (int)\n",
    "        :param num_label: Number of the label class (Label should start from 0)\n",
    "        :param padding_idx: The padding indexes (int)\n",
    "        :param dropout: The dropout for regularisation (Float)\n",
    "        :param bidirectional: Bool\n",
    "        :param device: Device type to use (GPU or CPU)\n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_label = num_label\n",
    "        self.padding_idx = padding_idx\n",
    "        self.bidirectional = bidirectional\n",
    "        self.device = device\n",
    "\n",
    "        # Layers initialisation\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=padding_idx)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=self.bidirectional,\n",
    "            dropout=0 if num_layers < 2 else dropout,\n",
    "            batch_first=True)  # LSTM layer\n",
    "\n",
    "        self.fc = nn.Linear(\n",
    "            self.hidden_dim * 2 if self.bidirectional else self.hidden_dim,\n",
    "            self.num_label\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        \"\"\"\n",
    "        Forward pass function\n",
    "        :param text: The text data\n",
    "        :param text_lengths: The length of the text\n",
    "        :return: Logits (Tensor float)\n",
    "        \"\"\"\n",
    "        # h0, c0 = self.zero_state(text.size(0))  # Initializes the zero state cell\n",
    "        res = self.embedding(text)  # Embedding layer\n",
    "        res = self.dropout(res)  # dropout layer after the embedding\n",
    "        res = nn.utils.rnn.pack_padded_sequence(res, text_lengths, batch_first=True)  # Pack the sequence\n",
    "        _, (hn, _) = self.lstm(res)  # We only need the last hidden state\n",
    "\n",
    "        if self.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hn[-2, :, :], hn[-1, :, :]), dim=1))\n",
    "        else:\n",
    "            hidden = self.dropout(hn[-1, :, :])\n",
    "\n",
    "        res = self.fc(hidden)  # The fully connected layer\n",
    "        return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_cnn(model, iterator, optimizer, criterion, scheduler=None):\n",
    "    total_loss,total_correct,total_prediction = 0.0, 0.0, 0.0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        # text,text_length = batch.AllCombined\n",
    "        logits = model(batch.AllCombined.cuda())\n",
    "        predictions = torch.max(logits, dim=-1)[1]\n",
    "        loss = criterion(logits, batch.label.cuda())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_correct += torch.eq(predictions, batch.label.cuda()).sum().item()\n",
    "        total_prediction += batch.label.size(0)\n",
    "    return total_loss / len(iterator),total_correct / total_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_lstm(model, iterator, optimizer, criterion, scheduler=None):\n",
    "    total_loss, total_correct, total_prediction = 0.0, 0.0, 0.0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        text, text_length = batch.AllCombined\n",
    "\n",
    "        logits = model(text.cuda(),text_length.cuda())\n",
    "        predictions = torch.max(logits,dim=-1)[1]\n",
    "        loss = criterion(logits,batch.label.cuda())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += torch.eq(predictions,batch.label.cuda()).sum().item()\n",
    "        total_prediction += batch.label.size(0)\n",
    "    return total_loss / len(iterator),total_correct / total_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cnn(model, iterator, criterion):\n",
    "    total_loss, total_correct, total_prediction = 0.0, 0.0, 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            logits = model(batch.AllCombined.cuda())\n",
    "            predictions = torch.max(logits, dim=-1)[1]\n",
    "            loss = criterion(logits, batch.label.cuda())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += torch.eq(predictions, batch.label.cuda()).sum().item()\n",
    "            total_prediction += batch.label.size(0)\n",
    "    return total_loss / len(iterator),total_correct / total_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lstm(model, iterator, criterion):\n",
    "    total_loss, total_correct, total_prediction = 0.0, 0.0, 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_length = batch.AllCombined\n",
    "\n",
    "            logits = model(text.cuda(), text_length.cuda())\n",
    "            predictions = torch.max(logits, dim=-1)[1]\n",
    "            loss = criterion(logits, batch.label.cuda())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += torch.eq(predictions, batch.label.cuda()).sum().item()\n",
    "            total_prediction += batch.label.size(0)\n",
    "    return total_loss / len(iterator), total_correct / total_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "SEED = 42\n",
    "# random.seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "reference = pd.read_csv(\"data/reference.csv\")\n",
    "sample = pd.read_csv(\"data/sample.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "text = pd.read_csv(\"data/text.csv\")\n",
    "label = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_df = data_preprocess(x=text, y=label, ref=reference)\n",
    "train_df.to_csv(\"train_df.csv\", index=None)\n",
    "train_df = pd.read_csv(\"train_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "test_preprocessed = data_preprocess(x=text,y=test,ref=reference)\n",
    "test_preprocessed.to_csv(\"test_prepared.csv\",index=None)\n",
    "test_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText vectors\n",
    "fast_text_vec = FastText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "TEXTcnn = data.Field(tokenize = spacy_tokenize, batch_first = True,sequential=True,lower=True)\n",
    "LABELcnn = data.LabelField(use_vocab=False,sequential=False)\n",
    "\n",
    "fields_cnn = [(\"id\",None),(\"label\",LABELcnn),(\"title\",None),(\"AllCombined\",TEXTcnn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# We import the train data file. Since it's in a csv file, we will use Tabular Dataset\n",
    "cnnDataset = torchtext.data.TabularDataset(\n",
    "    path=\"train_df.csv\",\n",
    "    format=\"CSV\",\n",
    "    fields=fields_cnn,\n",
    "    skip_header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train_cnn, X_val_cnn = cnnDataset.split(\n",
    "    split_ratio=0.2,\n",
    "    random_state=random.seed(SEED)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "TEXTcnn.build_vocab(\n",
    "    X_train_cnn,\n",
    "    vectors = fast_text_vec,\n",
    "    unk_init = torch.Tensor.normal_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXTcnn.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 256\n",
    "FILTER_SIZES = [1,2,3,4,5]\n",
    "OUTPUT_DIM = 5\n",
    "DROPOUT = 0.3\n",
    "DEVICE = torch.device('cuda')\n",
    "BATCH_SIZE = 5\n",
    "EPOCHS = 25\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCNN = CNNClassifier(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCNN.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_iter_cnn, X_val_iter_cnn = data.BucketIterator.splits(\n",
    "    (X_train_cnn, X_val_cnn),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.AllCombined),\n",
    "    sort_within_batch=True,\n",
    "    device = DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Transfer the FastText pretrained embedding\n",
    "UNK_IDX_CNN = TEXTcnn.vocab.stoi[TEXTcnn.unk_token]\n",
    "PAD_IDX_CNN = TEXTcnn.vocab.stoi[TEXTcnn.pad_token]\n",
    "\n",
    "pretrained_embeddings = TEXTcnn.vocab.vectors\n",
    "ModelCNN.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "UNK_IDX_CNN = TEXTcnn.vocab.stoi[TEXTcnn.unk_token]\n",
    "\n",
    "ModelCNN.embedding.weight.data[UNK_IDX_CNN] = torch.zeros(EMBEDDING_DIM)\n",
    "ModelCNN.embedding.weight.data[PAD_IDX_CNN] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ModelCNN.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(ModelCNN.parameters(), lr=LR)\n",
    "\n",
    "# Scheduler for the optimizer\n",
    "total_steps = len(X_train_iter_cnn) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = total_steps\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "best_valid_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "# model 2 100 x 300, 2 layers, unidirectional --> 80.9\n",
    "PATH = f\"ModelCNN.pt\" #50.8 %\n",
    "if os.path.exists(PATH):\n",
    "    print(\"Loading model from last checkpoint...\")\n",
    "    state = torch.load(PATH)\n",
    "    ModelCNN.load_state_dict(state['best_state_dict'])\n",
    "    best_valid_acc = state['best_valid_acc']\n",
    "    best_epoch = state['epoch']\n",
    "    has_checkpoint = True\n",
    "print(f\"Best Validations Accuracy so far: {best_valid_acc:.3f} at Epoch {best_epoch}\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    real_epoch = best_epoch + epoch\n",
    "    \n",
    "    train_loss, train_acc = train(ModelCNN, X_train_iter_cnn, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(ModelCNN, X_val_iter_cnn, criterion)\n",
    "    \n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_state_dict = copy.deepcopy(ModelCNN.state_dict())\n",
    "        print(f'Epoch {real_epoch: <{5}} | Train loss {train_loss:8.3f}| Train acc {train_acc:8.3f} | Valid loss {valid_loss:8.3f} | Valid acc {valid_acc:8.3f} | + ')\n",
    "        \n",
    "        #if not os.path.exists('Models'):\n",
    "        #    os.makedirs('Models')\n",
    "            # Let's create the checkpoint data to save\n",
    "        checkpoint = {\n",
    "            'epoch': real_epoch,\n",
    "            'best_valid_acc': best_valid_acc,\n",
    "            'best_state_dict': best_state_dict,\n",
    "            'embedding_dim':ModelCNN.embedding,\n",
    "            'n_filters':ModelCNN.n_filters,\n",
    "            'filter_sizes':ModelCNN.filter_sizes\n",
    "        }\n",
    "        torch.save(checkpoint, PATH)\n",
    "    else:\n",
    "        print(f'Epoch {real_epoch: <{5}} | Train loss {train_loss:8.3f}| Train acc {train_acc:8.3f} | Valid loss {valid_loss:8.3f} | Valid acc {valid_acc:8.3f} |')\n",
    "\n",
    "print(f\"The best Model Accuracy: {best_valid_acc:.3f}\")\n",
    "print(\"The best Model has been saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## CNN Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "test_preprocessed = data_preprocess(x=text,y=test,ref=reference)\n",
    "test_preprocessed.to_csv(\"test_prepared.csv\",index=None)\n",
    "test_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the best CNN models save from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "state = torch.load(PATH)\n",
    "ModelCNN.load_state_dict(state['best_state_dict'])\n",
    "best_valid_acc = state['best_valid_acc']\n",
    "print(best_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "_, acc = evaluate(ModelCNN, X_val_iter_cnn, criterion)\n",
    "print(f\"CNN Validation Accuracy: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make CNN Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def predict_lstm(test,field, model):\n",
    "    model.eval()\n",
    "    processed = field.process([field.preprocess(test)])\n",
    "    text,len_text = processed\n",
    "    preds = model(text.cuda(),len_text).argmax().item()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict_cnn(model, field, text):\n",
    "    model.eval()\n",
    "    text = field.preprocess(text)\n",
    "    text = field.process([text])\n",
    "    x = torch.tensor(text)\n",
    "    x = x.cuda()\n",
    "    logits = model(x)\n",
    "    y_pred = torch.max(logits, dim=-1)[1]\n",
    "    y_pred = y_pred.item()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cnn_preds = [[test_preprocessed.id[i],predict_cnn(ModelCNN, TEXTcnn, test_preprocessed.AllCombined[i])] for i in range(len(test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "submission_cnn = pd.DataFrame(data=cnn_preds,columns=['id','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(f\"CNN Prediction length: {submission_cnn.shape[0]}\")\n",
    "print(f\"Unique Labels Predicted: {list(submission_cnn.label.unique())}\")\n",
    "submission_cnn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "submission_cnn.to_csv(\"submission_cnn.csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model for the text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Fields definition\n",
    "TEXT_lstm = data.Field(\n",
    "    sequential=True,\n",
    "    lower=True,\n",
    "    use_vocab=True,\n",
    "    preprocessing=generate_n_grams,\n",
    "    tokenize=spacy_tokenize,\n",
    "    batch_first=True,\n",
    "    include_lengths=True\n",
    ")\n",
    "LABEL_lstm = data.LabelField(sequential=False,use_vocab=False)\n",
    "\n",
    "# We define the fields we need in our dataset file we need for the analysis\n",
    "fields_lstm = [\n",
    "    (\"id\",None),\n",
    "    (\"label\",LABEL_lstm),\n",
    "    (\"title\",None),\n",
    "    (\"AllCombined\",TEXT_lstm)\n",
    "]\n",
    "\n",
    "# We import the train data file. Since it's in a csv file, we will use Tabular Dataset\n",
    "textDataset = torchtext.data.TabularDataset(\n",
    "    path=\"train_df.csv\", \n",
    "    format=\"CSV\",\n",
    "    fields=fields_lstm,\n",
    "    skip_header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Dataset splitting\n",
    "X_train_lstm, X_val_lstm = textDataset.split(\n",
    "    split_ratio=0.2,\n",
    "    random_state=random.seed(SEED)\n",
    ")\n",
    "\n",
    "# Let's build our vocab on the training set only with Glove 6B tokens and 300d vectors\n",
    "# MAX_VOCAB_SIZE = 50000\n",
    "TEXT_lstm.build_vocab(\n",
    "    X_train_lstm,\n",
    "    vectors=fast_text_vec,\n",
    "    unk_init=torch.Tensor.normal_\n",
    ")\n",
    "\n",
    "# Find the padding index and the vocab size\n",
    "padding_idx = TEXT_lstm.vocab.stoi[\"<pad>\"]\n",
    "vocab_size = len(TEXT_lstm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Let's create ou iterators\n",
    "X_train_iter_lstm, X_val_iter_lstm = data.BucketIterator.splits(\n",
    "    (X_train_lstm,X_val_lstm),\n",
    "    batch_sizes=(32,32),\n",
    "    sort_within_batch = True,\n",
    "    sort_key=lambda x: len(x.AllCombined),\n",
    "    device=torch.device('cuda')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ModelLSTM = LSTMClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    num_layers=4,\n",
    "    embedding_dim=300,\n",
    "    hidden_dim=250,\n",
    "    num_label=5,\n",
    "    padding_idx=padding_idx,\n",
    "    dropout=0.4,\n",
    "    bidirectional=True,\n",
    "    device=torch.device('cuda')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#copying pre-trained word embeddings\n",
    "pretrained_embeddings = TEXT_lstm.vocab.vectors\n",
    "ModelLSTM.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT_lstm.vocab.stoi[TEXT_lstm.unk_token]\n",
    "ModelLSTM.embedding.weight.data[UNK_IDX] = torch.zeros(300)\n",
    "ModelLSTM.embedding.weight.data[padding_idx] = torch.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "ModelLSTM.cuda()\n",
    "LR = 1e-4\n",
    "optimizer = Adam(ModelLSTM.parameters(), lr=LR)\n",
    "\n",
    "total_steps = len(X_train_iter_lstm) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = total_steps\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "best_valid_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "# model 2 100 x 300, 2 layers, unidirectional --> 80.9\n",
    "PATH = f\"ModelLSTM.pt\" #82.9 %\n",
    "if os.path.exists(PATH):\n",
    "    print(\"Loading model from last checkpoint...\")\n",
    "    state = torch.load(PATH)\n",
    "    ModelLSTM.load_state_dict(state['best_state_dict'])\n",
    "    best_valid_acc = state['best_valid_acc']\n",
    "    best_epoch = state['epoch']\n",
    "    has_checkpoint = True\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    real_epoch = best_epoch + epoch\n",
    "    \n",
    "    train_loss, train_acc = train_lstm(ModelLSTM, X_train_iter_lstm, optimizer, criterion, scheduler=None)\n",
    "    valid_loss, valid_acc = evaluate_lstm(ModelLSTM, X_val_iter_lstm, criterion)\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_state_dict = copy.deepcopy(ModelLSTM.state_dict())\n",
    "        print(f'Epoch {real_epoch: <{5}} | Train loss {train_loss:8.3f}| Train acc {train_acc:8.3f} | Valid loss {valid_loss:8.3f} | Valid acc {valid_acc:8.3f} | + ')\n",
    "        \n",
    "\n",
    "        # Let's create the checkpoint data to save\n",
    "        checkpoint = {\n",
    "            'epoch': real_epoch,\n",
    "            'best_valid_acc': best_valid_acc,\n",
    "            'best_state_dict': best_state_dict,\n",
    "            'vocab_size':ModelLSTM.vocab_size,\n",
    "            'embedding_dim':ModelLSTM.embedding_dim,\n",
    "            'num_layers':ModelLSTM.num_layers,\n",
    "            'bidirectional':ModelLSTM.bidirectional,\n",
    "            'hidden_dim': ModelLSTM.hidden_dim\n",
    "        }\n",
    "        torch.save(checkpoint, PATH)\n",
    "    else:\n",
    "        print(f'Epoch {real_epoch: <{5}} | Train loss {train_loss:8.3f}| Train acc {train_acc:8.3f} | Valid loss {valid_loss:8.3f} | Valid acc {valid_acc:8.3f} |')\n",
    "    \n",
    "    # Tensorboard section\n",
    "print(f\"The best Model Accuracy: {best_valid_acc:.3f}\")\n",
    "print(\"The best Model has been saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "PATH = f\"ModelLSTM.pt\" #82.9 %\n",
    "state = torch.load(PATH)\n",
    "ModelLSTM.load_state_dict(state['best_state_dict'])\n",
    "best_valid_acc = state['best_valid_acc']\n",
    "print(best_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lstm(test,field, model):\n",
    "    model.eval()\n",
    "    processed = field.process([field.preprocess(test)])\n",
    "    text,len_text = processed\n",
    "    preds = model(text.cuda(),len_text).argmax().item()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_preds = [[test_preprocessed.id[i],predict_lstm(test_preprocessed.AllCombined[i],TEXT_lstm,ModelLSTM)] for i in range(len(test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_lstm = pd.DataFrame(data=lstm_preds,columns=['id','label'])\n",
    "submission_lstm.to_csv('ssubmission_lstm',index=None)\n",
    "submission_lstm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset propcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClfDataset:\n",
    "    def __init__(self, obs_id, text, label, tokenizer, max_len=512):\n",
    "        \"\"\"\n",
    "        Class for preparing text data for text classification / sentiment analysis task with BERT\n",
    "        :param text: Text to process\n",
    "        :param tokenizer: Text tokenizer\n",
    "        :param max_len: The max len for the padding\n",
    "        :param label: The label to predict in the task\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.id = obs_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.text[item])\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        # padding\n",
    "        padding_len = self.max_len - len(input_ids)\n",
    "        input_ids = input_ids + ([0] * padding_len)\n",
    "        mask = mask + ([0] * padding_len)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "        \n",
    "        if self.label is not None:\n",
    "            out = {\n",
    "                \"input_ids\": torch.tensor(input_ids),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids),\n",
    "                \"obs_id\": torch.tensor(self.id[item]),\n",
    "                \"label\": torch.tensor(self.label[item])\n",
    "            }\n",
    "            return out\n",
    "        else:\n",
    "            out = {\n",
    "                \"input_ids\": torch.tensor(input_ids),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids),\n",
    "                \"obs_id\": torch.tensor(self.id[item])\n",
    "            } \n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_path, dropout, n_class):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert_path = bert_path\n",
    "        self.n_class = n_class\n",
    "        self.bert = BertModel.from_pretrained(self.bert_path)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, self.n_class)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _, out = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(data_loader, model, optimizer, device, criterion, scheduler=None):\n",
    "    total_loss, total_correct, total_prediction = 0.0, 0.0, 0.0\n",
    "    model.train()\n",
    "    for bi, d in enumerate(data_loader):\n",
    "        # Unpack the training batch\n",
    "        ids = d[\"input_ids\"].to(device)\n",
    "        mask = d[\"mask\"].to(device)\n",
    "        token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "        label = d[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids=ids, \n",
    "            attention_mask=mask, \n",
    "            token_type_ids=token_type_ids, \n",
    "            labels=label\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        predictions = torch.max(logits, dim=-1)[1]\n",
    "        # loss = criterion(logits,label)\n",
    "        loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += torch.eq(predictions, label).sum().item()\n",
    "        total_prediction += label.size(0)\n",
    "    avg_train_loss = total_loss / len(data_loader)  # Average train loss over the all the batches\n",
    "    avg_train_acc = total_correct / total_prediction  # Averag train accuracy over the whole train set\n",
    "    return avg_train_loss, avg_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bert(data_loader, model, device, criterion):\n",
    "    total_loss, total_correct, total_prediction = 0.0, 0.0, 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for bi, d in enumerate(data_loader):\n",
    "            ids = d[\"input_ids\"].to(device)\n",
    "            mask = d[\"mask\"].to(device)\n",
    "            token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "            label = d[\"label\"].to(device)\n",
    "           \n",
    "            outputs = model(\n",
    "                input_ids=ids, \n",
    "                attention_mask=mask, \n",
    "                token_type_ids=token_type_ids, \n",
    "                labels=label\n",
    "            )\n",
    "            \n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            \n",
    "            predictions = torch.max(logits, dim=-1)[1]\n",
    "            # loss = criterion(logits, label)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_correct += torch.eq(predictions, label).sum().item()\n",
    "            total_prediction += label.size(0)\n",
    "    avg_valid_loss = total_loss / len(data_loader)\n",
    "    avg_valid_acc = total_correct / total_prediction\n",
    "    return avg_valid_loss, avg_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    MAX_LEN = 512\n",
    "    TRAIN_BATCH_SIZE = 10\n",
    "    EPOCHS = 5\n",
    "    DROPOUT = 0.4\n",
    "\n",
    "    df = pd.read_csv(\"train_df.csv\", usecols=['label', 'title','AllCombined'])\n",
    "    df_train, df_valid = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    df_train.reset_index(drop=True)\n",
    "    df_valid.reset_index(drop=True)\n",
    "\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    train_dataset = TextClfDataset(\n",
    "        obs_id = df_train.id.values,\n",
    "        text=df_train.AllCombined.values,\n",
    "        label=df_train.label.values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=RandomSampler(train_dataset),\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    valid_dataset = TextClfDataset(\n",
    "        obs_id = df_train.id.values,\n",
    "        text=df_valid.AllCombined.values,\n",
    "        label=df_valid.label.values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        sampler=SequentialSampler(valid_dataset),\n",
    "        batch_size=TRAIN_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    num_train_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE)\n",
    "    # model = BERTClassifier(dropout=DROPOUT)\n",
    "    ModelBERT = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=5)\n",
    "    #ModelBERT = BertClassifier(\n",
    "    #    \"bert-base-uncased\",\n",
    "    #    dropout=0.4,\n",
    "    #    n_class=5\n",
    "    #)\n",
    "    \n",
    "    ModelBERT.cuda()\n",
    "    \n",
    "    print(\n",
    "        f\"Nbr of parameters before freezing bert layers: \"\n",
    "        f\"{sum(p.numel() for p in ModelBERT.parameters() if p.requires_grad)}\"\n",
    "    )\n",
    "    # We freeze all the bert layers (encoder and embeddings layers)\n",
    "    for name, param in ModelBERT.named_parameters():\n",
    "        if name.startswith('bert'):\n",
    "            param.requires_grad = False\n",
    "    print(\n",
    "        f\"Nbr of parameters After freezing bert layers: \"\n",
    "        f\"{sum(p.numel() for p in ModelBERT.parameters() if p.requires_grad)}\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion.to(device)\n",
    "    lr = 2e-3\n",
    "    optimizer = AdamW(ModelBERT.parameters(), eps = 1e-8, lr=lr)\n",
    "    total_steps = len(train_data_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_valid_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    PATH = f\"ModelBERT.pt\" #50.8 %\n",
    "    if os.path.exists(PATH):\n",
    "        print(\"Loading model from last checkpoint...\")\n",
    "        state = torch.load(PATH)\n",
    "        ModelBERT.load_state_dict(state['best_state_dict'])\n",
    "        best_valid_acc = state['best_valid_acc']\n",
    "        best_epoch = state['epoch']\n",
    "        has_checkpoint = True\n",
    "    print(f\"Best Validations Accuracy so far: {best_valid_acc:.3f} at Epoch {best_epoch}\\n\")\n",
    "\n",
    "    # Measure the total training time for the whÃ¹ole run.\n",
    "    total_t0 = time.time()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        real_epoch = best_epoch + epoch\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train_bert(train_data_loader, ModelBERT, optimizer, device, criterion, scheduler)\n",
    "        valid_loss, valid_acc = eval_bert(valid_data_loader, ModelBERT, device, criterion)\n",
    "        \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "        \n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            best_state_dict = copy.deepcopy(ModelBERT.state_dict())\n",
    "            print(f'Epoch {real_epoch: <{5}} | Elapsed Time {training_time: <{10}} | Train loss {train_loss:8.3f}| Train acc {train_acc:8.3f} | Valid loss {valid_loss:8.3f} | Valid acc {valid_acc:8.3f} | + ')\n",
    "\n",
    "            # Let's create the checkpoint data to save\n",
    "            checkpoint = {\n",
    "                'epoch': real_epoch,\n",
    "                'best_valid_acc': best_valid_acc,\n",
    "                'best_state_dict': best_state_dict,\n",
    "            }\n",
    "            torch.save(checkpoint, PATH)\n",
    "        else:\n",
    "            print(f'Epoch {real_epoch: <{5}} | Elapsed Time {training_time: <{10}} | Train loss {train_loss:8.3f}| Train acc {train_acc:8.3f} | Valid loss {valid_loss:8.3f} | Valid acc {valid_acc:8.3f} |')\n",
    "    \n",
    "    # End of training\n",
    "    print(f\"The best Model Accuracy: {best_valid_acc:3.3f}\")\n",
    "    print(\"The best Model has been saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelBERT = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=5)\n",
    "PATH = f\"ModelBERT.pt\" #82.9 %\n",
    "state = torch.load(PATH)\n",
    "ModelBERT.load_state_dict(state['best_state_dict'])\n",
    "best_valid_acc = state['best_valid_acc']\n",
    "print(best_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bert(data_loader, model, device):\n",
    "    y_pred, pred_ids = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for bi, d in enumerate(data_loader):\n",
    "            print(f\"Step {bi}...\")\n",
    "            ids = d[\"input_ids\"].to(device)\n",
    "            mask = d[\"mask\"].to(device)\n",
    "            obs_ids = d[\"obs_id\"].to(device)\n",
    "            token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "            \n",
    "            obs_ids = obs_ids.to('cpu').numpy().tolist()\n",
    "            pred_ids.append(obs_ids)\n",
    "            \n",
    "            output = model(\n",
    "                input_ids=ids, \n",
    "                attention_mask=mask, \n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            logits = output[0]\n",
    "            \n",
    "            preds = torch.max(logits, dim=1)[1]\n",
    "            preds = preds.detach().cpu().numpy().tolist()\n",
    "            y_pred.append(preds)\n",
    "            \n",
    "    return (y_pred, pred_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nestings(ls):\n",
    "    \"\"\"\n",
    "    Function to flattern a nested list of 2 levels\n",
    "    :param ls: Nested List\n",
    "    :return: Flattern List\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for i in ls:\n",
    "        for j in i:\n",
    "            output.append(j)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(df_test, model):\n",
    "    MAX_LEN = 512\n",
    "    TEST_BATCH_SIZE = 300\n",
    "    \n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    test_dataset = TextClfDataset(\n",
    "        obs_id = df_test.id.values,\n",
    "        text=df_test.AllCombined.values,\n",
    "        label=None,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "    test_data_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        sampler=RandomSampler(test_dataset),\n",
    "        batch_size=TEST_BATCH_SIZE,\n",
    "    )\n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    model.cuda()\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    print(f\"Total Step: {len(test_data_loader)}\")\n",
    "    \n",
    "    y_pred, pred_ids = predict_bert(test_data_loader, model, DEVICE)\n",
    "    prediction_time = format_time(time.time() - t0)\n",
    "    y_pred = remove_nestings(y_pred)  # Flattern the nested list\n",
    "    pred_ids = remove_nestings(pred_ids)\n",
    "    print(f\"Elapsed {prediction_time: <{6}} | Test Data Size {df_test.shape} | Predictions List Size {len(y_pred)} | IDs List Size {len(pred_ids)}\")\n",
    "    \n",
    "    return (y_pred, pred_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = predict_test(test_preprocessed,ModelBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final = [[out[1][i], out[0][i]] for i in range(len(out[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_bert = pd.DataFrame(data=y_pred_final,columns=['id','label'])\n",
    "submission_bert.to_csv('submission_bert',index=None)\n",
    "submission_bert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
